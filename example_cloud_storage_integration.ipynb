{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Storage Integration Example\n",
    "\n",
    "This notebook demonstrates how to use Google Cloud Storage with the `colab_env` environment for data storage and retrieval.\n",
    "\n",
    "## Prerequisites\n",
    "1. Environment activated: `mamba activate colab_env`\n",
    "2. Google Cloud authentication completed\n",
    "3. PROJECT_ID environment variable set\n",
    "4. At least one Cloud Storage bucket created\n",
    "\n",
    "## Setup Instructions\n",
    "```bash\n",
    "# In terminal:\n",
    "export PROJECT_ID=\"your-project-id\"\n",
    "gcloud auth application-default login\n",
    "gsutil mb gs://your-unique-bucket-name/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Set up project ID\n",
    "PROJECT_ID = os.environ.get('PROJECT_ID')\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = input(\"Enter your Google Cloud Project ID: \")\n",
    "\n",
    "print(f\"Using project: {PROJECT_ID}\")\n",
    "\n",
    "# Initialize Cloud Storage client\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "print(\"Cloud Storage client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucket Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all buckets in your project\n",
    "print(\"Available buckets:\")\n",
    "buckets = []\n",
    "try:\n",
    "    for bucket in client.list_buckets():\n",
    "        buckets.append(bucket.name)\n",
    "        print(f\"  - {bucket.name} (created: {bucket.time_created.strftime('%Y-%m-%d')})\")\n",
    "    \n",
    "    if not buckets:\n",
    "        print(\"  No buckets found. You may need to create one first.\")\n",
    "        print(\"  Use: gsutil mb gs://your-unique-bucket-name/\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing buckets: {e}\")\n",
    "    buckets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select or create a bucket for this example\n",
    "if buckets:\n",
    "    BUCKET_NAME = buckets[0]  # Use first available bucket\n",
    "    print(f\"Using existing bucket: {BUCKET_NAME}\")\n",
    "else:\n",
    "    # Create a new bucket (uncomment and modify as needed)\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}-data-analysis-{datetime.now().strftime('%Y%m%d')}\"\n",
    "    print(f\"Creating new bucket: {BUCKET_NAME}\")\n",
    "    \n",
    "    # Uncomment to create bucket:\n",
    "    # try:\n",
    "    #     bucket = client.create_bucket(BUCKET_NAME, location=\"US\")\n",
    "    #     print(f\"Bucket {bucket.name} created successfully\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error creating bucket: {e}\")\n",
    "    #     BUCKET_NAME = input(\"Enter an existing bucket name: \")\n",
    "\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "print(f\"Working with bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample financial data for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample stock data\n",
    "dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='D')\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "\n",
    "stock_data = []\n",
    "for symbol in symbols:\n",
    "    base_price = np.random.uniform(100, 300)\n",
    "    prices = []\n",
    "    current_price = base_price\n",
    "    \n",
    "    for date in dates:\n",
    "        # Simulate price movement\n",
    "        daily_change = np.random.normal(0, 0.02)  # 2% daily volatility\n",
    "        current_price *= (1 + daily_change)\n",
    "        prices.append(current_price)\n",
    "        \n",
    "        stock_data.append({\n",
    "            'date': date,\n",
    "            'symbol': symbol,\n",
    "            'price': current_price,\n",
    "            'volume': np.random.randint(1000000, 50000000),\n",
    "            'market_cap': current_price * np.random.uniform(1e9, 3e12)\n",
    "        })\n",
    "\n",
    "df_stocks = pd.DataFrame(stock_data)\n",
    "print(f\"Created sample dataset with {len(df_stocks)} rows\")\n",
    "print(\"\\nSample data:\")\n",
    "df_stocks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Data to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Upload CSV file\n",
    "csv_data = df_stocks.to_csv(index=False)\n",
    "blob_csv = bucket.blob('data/stock_data.csv')\n",
    "\n",
    "try:\n",
    "    blob_csv.upload_from_string(csv_data, content_type='text/csv')\n",
    "    print(f\"‚úÖ CSV uploaded to gs://{BUCKET_NAME}/data/stock_data.csv\")\n",
    "    print(f\"   Size: {len(csv_data):,} bytes\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error uploading CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Upload Parquet file (more efficient for large datasets)\n",
    "parquet_buffer = io.BytesIO()\n",
    "df_stocks.to_parquet(parquet_buffer, index=False)\n",
    "parquet_data = parquet_buffer.getvalue()\n",
    "\n",
    "blob_parquet = bucket.blob('data/stock_data.parquet')\n",
    "\n",
    "try:\n",
    "    blob_parquet.upload_from_string(parquet_data, content_type='application/octet-stream')\n",
    "    print(f\"‚úÖ Parquet uploaded to gs://{BUCKET_NAME}/data/stock_data.parquet\")\n",
    "    print(f\"   Size: {len(parquet_data):,} bytes\")\n",
    "    print(f\"   Compression ratio: {len(csv_data) / len(parquet_data):.1f}x smaller\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error uploading Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Upload JSON metadata\n",
    "metadata = {\n",
    "    'dataset_name': 'sample_stock_data',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'symbols': symbols,\n",
    "    'date_range': {\n",
    "        'start': dates[0].isoformat(),\n",
    "        'end': dates[-1].isoformat()\n",
    "    },\n",
    "    'record_count': len(df_stocks),\n",
    "    'columns': list(df_stocks.columns)\n",
    "}\n",
    "\n",
    "blob_metadata = bucket.blob('metadata/stock_data_metadata.json')\n",
    "\n",
    "try:\n",
    "    blob_metadata.upload_from_string(\n",
    "        json.dumps(metadata, indent=2), \n",
    "        content_type='application/json'\n",
    "    )\n",
    "    print(f\"‚úÖ Metadata uploaded to gs://{BUCKET_NAME}/metadata/stock_data_metadata.json\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error uploading metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Managing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the bucket\n",
    "print(f\"Files in bucket '{BUCKET_NAME}':\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    blobs = list(bucket.list_blobs())\n",
    "    \n",
    "    if blobs:\n",
    "        for blob in blobs:\n",
    "            size_mb = blob.size / (1024 * 1024) if blob.size else 0\n",
    "            print(f\"üìÑ {blob.name}\")\n",
    "            print(f\"   Size: {size_mb:.2f} MB\")\n",
    "            print(f\"   Created: {blob.time_created.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"   Content-Type: {blob.content_type}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No files found in bucket\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error listing files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Read CSV directly from Cloud Storage\n",
    "print(\"Reading CSV from Cloud Storage...\")\n",
    "try:\n",
    "    df_from_csv = pd.read_csv(f'gs://{BUCKET_NAME}/data/stock_data.csv')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df_from_csv)} rows from CSV\")\n",
    "    print(\"First few rows:\")\n",
    "    display(df_from_csv.head())\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Read Parquet (faster for large files)\n",
    "print(\"Reading Parquet from Cloud Storage...\")\n",
    "try:\n",
    "    df_from_parquet = pd.read_parquet(f'gs://{BUCKET_NAME}/data/stock_data.parquet')\n",
    "    print(f\"‚úÖ Successfully loaded {len(df_from_parquet)} rows from Parquet\")\n",
    "    \n",
    "    # Verify data integrity\n",
    "    if df_from_parquet.equals(df_from_csv):\n",
    "        print(\"‚úÖ Data integrity verified - CSV and Parquet match\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: CSV and Parquet data don't match\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Download and read metadata\n",
    "print(\"Reading metadata from Cloud Storage...\")\n",
    "try:\n",
    "    blob_metadata = bucket.blob('metadata/stock_data_metadata.json')\n",
    "    metadata_content = blob_metadata.download_as_text()\n",
    "    metadata_loaded = json.loads(metadata_content)\n",
    "    \n",
    "    print(\"‚úÖ Metadata loaded successfully:\")\n",
    "    for key, value in metadata_loaded.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis with Cloud Storage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis on the data loaded from Cloud Storage\n",
    "df = df_from_parquet.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Calculate returns\n",
    "df = df.sort_values(['symbol', 'date'])\n",
    "df['daily_return'] = df.groupby('symbol')['price'].pct_change()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Stock Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = df.groupby('symbol').agg({\n",
    "    'price': ['first', 'last', 'min', 'max'],\n",
    "    'daily_return': ['mean', 'std'],\n",
    "    'volume': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['Start_Price', 'End_Price', 'Min_Price', 'Max_Price', \n",
    "                  'Avg_Return', 'Volatility', 'Avg_Volume']\n",
    "\n",
    "# Calculate total return\n",
    "summary['Total_Return'] = ((summary['End_Price'] / summary['Start_Price']) - 1) * 100\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Price trends\n",
    "plt.subplot(2, 2, 1)\n",
    "for symbol in symbols:\n",
    "    symbol_data = df[df['symbol'] == symbol]\n",
    "    plt.plot(symbol_data['date'], symbol_data['price'], label=symbol, linewidth=2)\n",
    "\n",
    "plt.title('Stock Price Trends')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Returns distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "for symbol in symbols:\n",
    "    symbol_returns = df[df['symbol'] == symbol]['daily_return'].dropna()\n",
    "    plt.hist(symbol_returns, alpha=0.6, bins=30, label=symbol)\n",
    "\n",
    "plt.title('Daily Returns Distribution')\n",
    "plt.xlabel('Daily Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "volatility_data = summary['Volatility'].sort_values(ascending=True)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(volatility_data)))\n",
    "bars = plt.barh(volatility_data.index, volatility_data.values, color=colors)\n",
    "plt.title('Volatility Comparison')\n",
    "plt.xlabel('Daily Return Standard Deviation')\n",
    "\n",
    "# Risk vs Return scatter\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(summary['Volatility'], summary['Total_Return'], \n",
    "           s=100, alpha=0.7, c=range(len(summary)), cmap='viridis')\n",
    "\n",
    "for i, symbol in enumerate(summary.index):\n",
    "    plt.annotate(symbol, \n",
    "                (summary.loc[symbol, 'Volatility'], summary.loc[symbol, 'Total_Return']),\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Risk vs Return')\n",
    "plt.xlabel('Volatility (Risk)')\n",
    "plt.ylabel('Total Return (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results back to Cloud Storage\n",
    "results = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'summary_statistics': summary.to_dict(),\n",
    "    'best_performer': summary['Total_Return'].idxmax(),\n",
    "    'worst_performer': summary['Total_Return'].idxmin(),\n",
    "    'highest_volatility': summary['Volatility'].idxmax(),\n",
    "    'lowest_volatility': summary['Volatility'].idxmin()\n",
    "}\n",
    "\n",
    "# Upload results\n",
    "try:\n",
    "    blob_results = bucket.blob(f'results/analysis_results_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.json')\n",
    "    blob_results.upload_from_string(\n",
    "        json.dumps(results, indent=2, default=str), \n",
    "        content_type='application/json'\n",
    "    )\n",
    "    print(f\"‚úÖ Analysis results saved to gs://{BUCKET_NAME}/{blob_results.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving results: {e}\")\n",
    "\n",
    "# Save the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "summary['Total_Return'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Total Returns by Stock')\n",
    "plt.ylabel('Return (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to buffer and upload\n",
    "try:\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_data = img_buffer.getvalue()\n",
    "    \n",
    "    blob_plot = bucket.blob(f'plots/returns_chart_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.png')\n",
    "    blob_plot.upload_from_string(img_data, content_type='image/png')\n",
    "    print(f\"‚úÖ Plot saved to gs://{BUCKET_NAME}/{blob_plot.name}\")\n",
    "    \n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Cloud Storage Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set lifecycle management (example)\n",
    "print(\"Bucket information and settings:\")\n",
    "print(f\"Bucket name: {bucket.name}\")\n",
    "print(f\"Location: {bucket.location}\")\n",
    "print(f\"Storage class: {bucket.storage_class}\")\n",
    "print(f\"Creation time: {bucket.time_created}\")\n",
    "\n",
    "# Check bucket size\n",
    "total_size = sum(blob.size for blob in bucket.list_blobs() if blob.size)\n",
    "print(f\"Total bucket size: {total_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signed URLs for sharing (optional)\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"Generating signed URLs for file sharing:\")\n",
    "try:\n",
    "    # Generate URL valid for 1 hour\n",
    "    blob_csv = bucket.blob('data/stock_data.csv')\n",
    "    url = blob_csv.generate_signed_url(\n",
    "        version=\"v4\",\n",
    "        expiration=timedelta(hours=1),\n",
    "        method=\"GET\"\n",
    "    )\n",
    "    print(f\"\\nüìé Signed URL for CSV file (valid for 1 hour):\")\n",
    "    print(f\"   {url[:100]}...\")\n",
    "    print(\"   ‚ö†Ô∏è This URL allows temporary access without authentication\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating signed URL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete test files (be careful!)\n",
    "# cleanup = input(\"Delete test files? (yes/no): \")\n",
    "# if cleanup.lower() == 'yes':\n",
    "#     try:\n",
    "#         blobs_to_delete = ['data/stock_data.csv', 'data/stock_data.parquet', \n",
    "#                           'metadata/stock_data_metadata.json']\n",
    "#         \n",
    "#         for blob_name in blobs_to_delete:\n",
    "#             blob = bucket.blob(blob_name)\n",
    "#             if blob.exists():\n",
    "#                 blob.delete()\n",
    "#                 print(f\"üóëÔ∏è Deleted {blob_name}\")\n",
    "#                 \n",
    "#         # Delete results and plots folders\n",
    "#         for blob in bucket.list_blobs(prefix='results/'):\n",
    "#             blob.delete()\n",
    "#             print(f\"üóëÔ∏è Deleted {blob.name}\")\n",
    "#             \n",
    "#         for blob in bucket.list_blobs(prefix='plots/'):\n",
    "#             blob.delete()\n",
    "#             print(f\"üóëÔ∏è Deleted {blob.name}\")\n",
    "#             \n",
    "#         print(\"‚úÖ Cleanup completed\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error during cleanup: {e}\")\n",
    "# else:\n",
    "#     print(\"Files preserved\")\n",
    "\n",
    "print(\"\\nüìÅ Current bucket contents:\")\n",
    "for blob in bucket.list_blobs():\n",
    "    print(f\"  üìÑ {blob.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### 1. **File Organization**\n",
    "- Use logical folder structure (`data/`, `results/`, `metadata/`, `plots/`)\n",
    "- Include timestamps in filenames for versioning\n",
    "- Use appropriate file formats (Parquet for large datasets, JSON for metadata)\n",
    "\n",
    "### 2. **Performance**\n",
    "- Use Parquet format for large datasets (better compression and faster reads)\n",
    "- Read data directly from GCS URLs when possible\n",
    "- Consider data partitioning for very large datasets\n",
    "\n",
    "### 3. **Cost Management**\n",
    "- Set up lifecycle policies to automatically delete or archive old files\n",
    "- Use appropriate storage classes (Standard, Nearline, Coldline, Archive)\n",
    "- Monitor storage costs regularly\n",
    "\n",
    "### 4. **Security**\n",
    "- Use IAM roles and permissions appropriately\n",
    "- Be careful with signed URLs and their expiration times\n",
    "- Never commit credentials to version control\n",
    "\n",
    "### 5. **Integration**\n",
    "- Combine with BigQuery for analysis of large datasets\n",
    "- Use with Cloud Functions for automated processing\n",
    "- Integrate with other GCP services as needed\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Explore bucket policies**: Set up lifecycle management and access controls\n",
    "2. **Automate workflows**: Use Cloud Functions or Cloud Run for scheduled data processing\n",
    "3. **Scale up**: Work with larger datasets and implement data partitioning\n",
    "4. **Integrate with BigQuery**: Load data from GCS to BigQuery for advanced analytics\n",
    "5. **Set monitoring**: Implement logging and monitoring for your data pipelines\n",
    "\n",
    "## Useful Resources\n",
    "\n",
    "- [Cloud Storage Documentation](https://cloud.google.com/storage/docs)\n",
    "- [Cloud Storage Python Client](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python)\n",
    "- [gsutil Command Reference](https://cloud.google.com/storage/docs/gsutil)\n",
    "- [Cloud Storage Pricing](https://cloud.google.com/storage/pricing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
